# ðŸ¤– Q-A Chatbot using Ollama

Q-A Chatbot using Ollama is a lightweight and powerful web-based chatbot interface that leverages **local large language models (LLMs)** with the help of **LangChain**, **Streamlit**, and **Ollama**. It allows users to interact with multiple local models in a conversational format through a clean and intuitive UI.

---

## ðŸš€ Features

### ðŸ” Authentication & Setup
- Uses `.env` for secure API key injection
- LangSmith tracing support for debugging and monitoring

### ðŸ§  LLM Support (via Ollama)
- gemma:2b
- llama3
- llama3:latest
- llama3.2:latest
- mxbai-embed-large:latest

### ðŸ—£ï¸ Prompt Handling & Response
- LangChain-powered prompt chaining
- Custom prompt template for consistent AI behavior
- Streaming response capability (can be extended)

### ðŸ’» Streamlit Web Interface
- User input for questions
- Model selection from sidebar
- Interactive Q&A output area

---

## ðŸ› ï¸ Tech Stack

### ðŸ”§ Backend
- **LangChain** â€“ For chaining prompts and model interaction
- **Ollama** â€“ Running local LLMs
- **Python** â€“ Core application logic
- **dotenv** â€“ Environment variable support

### ðŸ’» Frontend
- **Streamlit** â€“ Lightweight web frontend for user interaction

---

## ðŸ“¦ Installation

### âœ… Prerequisites
- Python 3.8+
- Ollama installed and running locally
- Models downloaded via `ollama pull`
- Docker (optional, if containerizing)

### ðŸ§° Setup Instructions

```bash
# 1. Clone the repository
git clone https://github.com/your-username/Q-A-Chatbot-using-Ollama.git
cd Q-A-Chatbot-using-Ollama

# 2. Create virtual environment
python -m venv venv

# 3. Activate the environment
# On Windows:
venv\Scripts\activate
# On Mac/Linux:
source venv/bin/activate

# 4. Install dependencies
pip install -r requirements.txt

# 5. Create a .env file
echo LANGSMITH_API_KEY=your_api_key_here > .env
